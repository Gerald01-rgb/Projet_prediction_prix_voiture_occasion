{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49b285-9721-4cea-9b95-47774100c6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc449b0-5fb5-402d-a634-c42c06dcdb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.Cr√©ation du pipeline\n",
    "def create_pipeline(X_train):\n",
    "    \"\"\"Pipeline am√©lior√© avec corrections des erreurs\"\"\"\n",
    "    \n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=42)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=f_regression, k='all'))\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    models = {\n",
    "        'Ridge': RidgeCV(alphas=np.logspace(-3, 3, 100)),\n",
    "        'Lasso': LassoCV(alphas=np.logspace(-4, 2, 50), cv=5, random_state=42),\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'DecisionTree': DecisionTreeRegressor(\n",
    "            max_depth=6,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'KNN': KNeighborsRegressor(n_neighbors=5),\n",
    "        \n",
    "        'HistGradientBoosting': HistGradientBoostingRegressor(\n",
    "            max_iter=200,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            min_samples_leaf=15,\n",
    "            l2_regularization=0.2,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBRegressor(\n",
    "            n_estimators=300,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            reg_alpha=0.2,\n",
    "            reg_lambda=1.0,\n",
    "            early_stopping_rounds=10,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # 5. Construction des pipelines optimis√©s\n",
    "    pipelines = {}\n",
    "    for name, model in models.items():\n",
    "        steps = [\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ]\n",
    "        \n",
    "        # Ajout de RFECV seulement pour les mod√®les lin√©aires\n",
    "        if name in ['Ridge', 'Lasso', 'LinearRegression']:\n",
    "            steps.insert(1, ('feature_selector', RFECV(\n",
    "                estimator=model,\n",
    "                step=5,\n",
    "                cv=3,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                n_jobs=-1\n",
    "            )))\n",
    "        \n",
    "        pipelines[name] = Pipeline(steps)\n",
    "    return pipelines, preprocessor, numeric_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f282a87-3836-4318-b6d5-734ad117f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Evaluation et optimisation des mod√®les\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_and_optimize_models(pipelines, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    √âvalue et optimise les mod√®les avec gestion sp√©ciale pour XGBoost,\n",
    "    correction pour v√©hicules premium et comparaison des performances.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    results = []\n",
    "    \n",
    "    print(\"üìä D√©but de l'√©valuation des mod√®les...\")\n",
    "\n",
    "    for name, pipeline in pipelines.items():\n",
    "        print(f\"\\nüîç Traitement du mod√®le {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Gestion sp√©ciale pour XGBoost\n",
    "            if 'XGB' in name or 'XGBoost' in name:\n",
    "                # 1. Cr√©er une copie du preprocessor\n",
    "                preprocessor = clone(pipeline.named_steps['preprocessor'])\n",
    "                \n",
    "                # 2. Pr√©parer les donn√©es transform√©es\n",
    "                X_train_transformed = preprocessor.fit_transform(X_train, y_train)\n",
    "                X_test_transformed = preprocessor.transform(X_test)\n",
    "                \n",
    "                # 3. Configurer XGBoost avec early stopping\n",
    "                model = XGBRegressor(\n",
    "                    n_estimators=1000,  # Nombre √©lev√© car early stopping activ√©\n",
    "                    max_depth=5,\n",
    "                    learning_rate=0.05,\n",
    "                    reg_alpha=0.1,\n",
    "                    reg_lambda=1.0,\n",
    "                    early_stopping_rounds=50,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                # 4. Entra√Ænement avec jeu de validation\n",
    "                model.fit(\n",
    "                    X_train_transformed, y_train,\n",
    "                    eval_set=[(X_test_transformed, y_test)],\n",
    "                    verbose=10  # Affiche les m√©triques toutes les 10 it√©rations\n",
    "                )\n",
    "                \n",
    "                # 5. Reconstruire le pipeline complet\n",
    "                pipeline.named_steps['preprocessor'] = preprocessor\n",
    "                pipeline.named_steps['model'] = model\n",
    "            else:\n",
    "                # Entra√Ænement standard pour les autres mod√®les\n",
    "                pipeline.fit(X_train, y_train)\n",
    "            \n",
    "                # Entra√Ænement du mod√®le\n",
    "                pipeline.fit(X_train, y_train)\n",
    "            \n",
    "                # Pr√©dictions de base\n",
    "                y_train_pred = pipeline.predict(X_train)\n",
    "                y_test_pred = pipeline.predict(X_test)\n",
    "            \n",
    "                \n",
    "            # Calcul des m√©triques\n",
    "            metrics = {\n",
    "                'Mod√®le': name,\n",
    "                'MAPE_train': mean_absolute_percentage_error(y_train, y_train_pred),\n",
    "                'MAPE_test': mean_absolute_percentage_error(y_test, y_test_pred),\n",
    "                'RMSE_train': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "                'MAE_train': mean_absolute_error(y_train, y_train_pred),\n",
    "                'R¬≤_train': r2_score(y_train, y_train_pred),\n",
    "                'RMSE_test': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "                'MAE_test': mean_absolute_error(y_test, y_test_pred),\n",
    "                'R¬≤_test': r2_score(y_test, y_test_pred),\n",
    "                'Type': type(pipeline.named_steps['model']).__name__\n",
    "            }\n",
    "            \n",
    "            # Stockage des r√©sultats\n",
    "            results.append(metrics)\n",
    "            all_results[name] = metrics\n",
    "            \n",
    "            print(f\"‚úÖ {name} - R¬≤ Test: {metrics['R¬≤_test']:.3f} | RMSE Test: {metrics['RMSE_test']:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur avec {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Cr√©ation du DataFrame des r√©sultats\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calcul du score agr√©g√© (plus bas = meilleur)\n",
    "    results_df['Score_agr√©g√©'] = (\n",
    "        results_df['RMSE_test'] + \n",
    "        results_df['MAE_test'] - \n",
    "        results_df['R¬≤_test']\n",
    "    )\n",
    "    \n",
    "    # S√©lection du meilleur mod√®le\n",
    "    best_idx = results_df['Score_agr√©g√©'].idxmin()\n",
    "    best_model_name = results_df.loc[best_idx, 'Mod√®le']\n",
    "    best_model_final = pipelines[best_model_name]\n",
    "    \n",
    "    # Affichage des r√©sultats\n",
    "    print(\"\\nüìã R√©sultats d√©taill√©s :\")\n",
    "    print(results_df.sort_values('Score_agr√©g√©').to_string(index=False))\n",
    "    \n",
    "    # Visualisation\n",
    "    plot_model_comparisons(results_df)\n",
    "    \n",
    "    return all_results, best_model_name, best_model_final\n",
    "\n",
    "def plot_model_comparisons(results_df):\n",
    "    \"\"\"Visualisation comparative des performances des mod√®les\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Tri des r√©sultats par R¬≤ test\n",
    "    results_df = results_df.sort_values('R¬≤_test', ascending=False)\n",
    "    \n",
    "    # Graphique √† barres pour R¬≤\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.barh(results_df['Mod√®le'], results_df['R¬≤_test'], color='skyblue')\n",
    "    plt.title('Comparaison des R¬≤ (Test)')\n",
    "    plt.xlim(0, 1)\n",
    "    \n",
    "    # Graphique √† barres pour RMSE\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.barh(results_df['Mod√®le'], results_df['RMSE_test'], color='lightgreen')\n",
    "    plt.title('Comparaison des RMSE (Test)')\n",
    "    \n",
    "    # Graphique √† barres pour MAE\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.barh(results_df['Mod√®le'], results_df['MAE_test'], color='salmon')\n",
    "    plt.title('Comparaison des MAE (Test)')\n",
    "    \n",
    "    # Graphique combin√©\n",
    "    plt.subplot(2, 2, 4)\n",
    "    width = 0.3\n",
    "    x = np.arange(len(results_df))\n",
    "    plt.bar(x - width, results_df['R¬≤_test'], width, label='R¬≤')\n",
    "    plt.bar(x, results_df['RMSE_test'], width, label='RMSE')\n",
    "    plt.bar(x + width, results_df['MAE_test'], width, label='MAE')\n",
    "    plt.xticks(x, results_df['Mod√®le'], rotation=45)\n",
    "    plt.legend()\n",
    "    plt.title('M√©triques combin√©es')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
