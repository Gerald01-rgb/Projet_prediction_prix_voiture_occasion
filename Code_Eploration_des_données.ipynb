{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5527d-ff39-4ae2-a119-88dd1d29032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yellowbrick\n",
    "import yellowbrick\n",
    "from pathlib import Path\n",
    "import pendulum\n",
    "import dill\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import clone \n",
    "from sklearn.feature_selection import RFECV\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import missingno as msno\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn import set_config\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OrdinalEncoder, OneHotEncoder\n",
    "import shap\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from scipy.stats import loguniform, uniform\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from loguru import logger\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ea499-1494-49be-9b6c-e5e71917dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chargement et analyse initiale des données\n",
    "def load_and_analyze_data(file_path):\n",
    "    \"\"\"\n",
    "    Charge les données et effectue une analyse initiale\n",
    "    \"\"\"\n",
    "   \n",
    "    data = pd.read_csv(file_path, na_values=['?'])\n",
    "    data.columns = data.columns.str.replace(\"-\", \"_\")\n",
    "\n",
    "    print(\"Informations sur le dataset:\")\n",
    "    print(data.info())\n",
    "    print(\"\\nAperçu des 5 premières lignes:\")\n",
    "    print(data.head())\n",
    "    print(\"\\nStatistiques descriptives:\")\n",
    "    print(data.describe(include='all'))\n",
    "    \n",
    "    missing_values = data.isnull().sum()\n",
    "    print(\"\\nValeurs manquantes par colonne:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199120e-83f1-45c8-9d6a-39a3c20f4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Analyse exploratoire des données (EDA)\n",
    "def perform_eda(data):\n",
    "    \"\"\"\n",
    "    Réalise une analyse exploratoire des données\n",
    "    \"\"\"\n",
    "    # Distribution du prix\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data['price'].dropna(), kde=True)\n",
    "    plt.title('Distribution du prix des voitures')\n",
    "    plt.xlabel('Prix')\n",
    "    plt.ylabel('Fréquence')\n",
    "    plt.savefig('price_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Relation entre variables numériques et prix\n",
    "    numerical_features = ['wheel_base', 'length', 'width', 'height', 'curb_weight', \n",
    "                         'engine_size', 'bore', 'stroke', 'compression_ratio', \n",
    "                         'horsepower', 'peak_rpm','symboling', 'city_mpg', 'highway_mpg']\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(numerical_features[:9]):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.scatter(data[feature], data['price'], alpha=0.5)\n",
    "        plt.title(f'Prix vs {feature}')\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Prix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('price_vs_features.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Matrice de corrélation\n",
    "    numeric_df = data.select_dtypes(include=['int64', 'float64'])\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Matrice de corrélation des variables numériques')\n",
    "    plt.savefig('correlation_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Distribution des prix par marque\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    brand_avg_price = data.groupby('make')['price'].mean().sort_values(ascending=False)\n",
    "    sns.barplot(x=brand_avg_price.index, y=brand_avg_price.values)\n",
    "    plt.title('Prix moyen par marque')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Marque')\n",
    "    plt.ylabel('Prix moyen')\n",
    "    plt.savefig('avg_price_by_brand.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return data, correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e574bdd4-72d8-407f-bedc-50ab39fc0884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Traitement des valeurs manquantes et aberrantes\n",
    "def handle_missing_and_outliers(data, price_col='price', threshold=0.7, show_plots=True):\n",
    "    \"\"\"\n",
    "    Traite les valeurs manquantes et aberrantes de manière complète\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame contenant les données\n",
    "        price_col (str): Nom de la colonne cible (prix)\n",
    "        threshold (float): Seuil pour supprimer les colonnes trop manquantes (0-1)\n",
    "        show_plots (bool): Afficher les visualisations si True\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame nettoyé\n",
    "        dict: Statistiques de nettoyage\n",
    "    \"\"\"\n",
    "    # Initialisation des statistiques\n",
    "    stats = {\n",
    "        'missing_before': data.isnull().sum().sum(),\n",
    "        'outliers_detected': {},\n",
    "        'columns_dropped': []\n",
    "    }\n",
    "    \n",
    "    # 1. Visualisation des valeurs manquantes\n",
    "    if show_plots:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        msno.matrix(data)\n",
    "        plt.title('Distribution des Valeurs Manquantes', pad=20)\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. Suppression des colonnes avec trop de valeurs manquantes\n",
    "    cols_to_drop = data.columns[data.isnull().mean() > threshold]\n",
    "    if not cols_to_drop.empty:\n",
    "        stats['columns_dropped'] = cols_to_drop.tolist()\n",
    "        data = data.drop(columns=cols_to_drop)\n",
    "        print(f\"Colonnes supprimées (>{threshold*100}% manquants): {list(cols_to_drop)}\")\n",
    "\n",
    "    \n",
    "    # 3. Détection et traitement des outliers pour toutes les variables numériques\n",
    "    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        q1 = data[col].quantile(0.25)\n",
    "        q3 = data[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        \n",
    "        # Détection des outliers\n",
    "        outliers_mask = (data[col] < lower) | (data[col] > upper)\n",
    "        n_outliers = outliers_mask.sum()\n",
    "        \n",
    "        if n_outliers > 0:\n",
    "            stats['outliers_detected'][col] = {\n",
    "                'count': n_outliers,\n",
    "                'percentage': n_outliers / len(data) * 100,\n",
    "                'method': 'IQR'\n",
    "            }\n",
    "            \n",
    "            # Remplacement des outliers par NaN\n",
    "            data[col] = np.where(outliers_mask, np.nan, data[col])\n",
    "    \n",
    "    # 4. Traitement spécial pour la variable cible (price)\n",
    "    if price_col in data.columns:\n",
    "        q1 = data[price_col].quantile(0.25)\n",
    "        q3 = data[price_col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        # Visualisation des outliers pour price\n",
    "        if show_plots:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.boxplot(x=data[price_col].dropna())\n",
    "            plt.title('Distribution du prix avec limites des outliers', pad=15)\n",
    "            plt.axvline(lower_bound, color='r', linestyle='--', label='Limite inférieure')\n",
    "            plt.axvline(upper_bound, color='r', linestyle='--', label='Limite supérieure')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            # Boxplot pour toutes les variables numériques\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            sns.boxplot(data=data[numeric_cols])\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.title('Distribution des variables numériques après traitement', pad=15)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # 5. Statistiques finales\n",
    "    stats['missing_after'] = data.isnull().sum().sum()\n",
    "    stats['remaining_columns'] = list(data.columns)\n",
    "    \n",
    "    print(f\"\\nRésumé du nettoyage:\")\n",
    "    print(f\"- Valeurs manquantes initiales: {stats['missing_before']}\")\n",
    "    print(f\"- Valeurs manquantes après traitement: {stats['missing_after']}\")\n",
    "    print(f\"- Colonnes supprimées: {stats['columns_dropped'] or 'Aucune'}\")\n",
    "    print(\"\\nDétection des outliers:\")\n",
    "    for col, info in stats['outliers_detected'].items():\n",
    "        print(f\"- {col}: {info['count']} outliers ({info['percentage']:.2f}%)\")\n",
    "    \n",
    "    # Sauvegarde des données nettoyées\n",
    "    data['price_original'] = data.get(price_col, np.nan)\n",
    "    return data, stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
